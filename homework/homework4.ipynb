{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1131b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing missing packages: ['beautifulsoup4']\n",
      "Installation finished.\n",
      "Created .env.example at c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\.env.example (use this to create your local .env)\n",
      "[2025-08-26T10:00:58.906969] Starting ingestion workflow\n",
      "[2025-08-26T10:00:58.908963] Using yfinance fallback for API pull\n",
      "[2025-08-26T10:00:59.694449] API DataFrame columns: ['timestamp', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "[2025-08-26T10:00:59.696647] yfinance fetch & validation OK\n",
      "[2025-08-26T10:00:59.708207] Saved API CSV to c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\raw\\api_yfinance_AAPL_20250826-1000.csv\n",
      "[2025-08-26T10:01:00.306170] Scrape validation OK\n",
      "[2025-08-26T10:01:00.309389] Saved scraped CSV to c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\raw\\scrape_wikipedia_sp500_constituents_20250826-1000.csv\n",
      "[2025-08-26T10:01:00.312100] Wrote ingestion documentation to c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\notebooks\\stage04_ingestion_notes.md\n",
      "[2025-08-26T10:01:00.313244] Ingestion workflow completed\n",
      "[2025-08-26T10:01:00.314244] Summary of outputs:\n",
      "[2025-08-26T10:01:00.314244]  - c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\raw\\api_diag_20250826-1000.txt\n",
      "[2025-08-26T10:01:00.314244]  - c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\raw\\api_yfinance_AAPL_20250826-1000.csv\n",
      "[2025-08-26T10:01:00.314244]  - c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\raw\\ingestion_log_20250826-1000.txt\n",
      "[2025-08-26T10:01:00.315499]  - c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\raw\\scrape_diag_20250826-1000.txt\n",
      "[2025-08-26T10:01:00.315499]  - c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\raw\\scrape_wikipedia_sp500_constituents_20250826-1000.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "stage04_ingest.py\n",
    "\n",
    "Single-file ingestion workflow for:\n",
    "- API pull (Alpha Vantage preferred; yfinance fallback)\n",
    "- Web scrape (Wikipedia S&P 500 constituents)\n",
    "- Validation, saving to data/raw/\n",
    "- Documentation written to notebooks/\n",
    "\n",
    "Usage:\n",
    "    python stage04_ingest.py\n",
    "\n",
    "Ensure you have an optional .env in the repo root with:\n",
    "    API_KEY=your_alpha_vantage_key\n",
    "    TICKER=AAPL\n",
    "If you do not create a .env, the script will fall back to yfinance.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import datetime\n",
    "import time\n",
    "import io\n",
    "import textwrap\n",
    "\n",
    "# ---------- Helper: ensure required packages ----------\n",
    "REQUIRED = [\"requests\", \"python-dotenv\", \"pandas\", \"beautifulsoup4\", \"yfinance\", \"lxml\"]\n",
    "def ensure_packages(pkgs):\n",
    "    import importlib\n",
    "    missing = []\n",
    "    for p in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(p if p != \"python-dotenv\" else \"dotenv\")\n",
    "        except Exception:\n",
    "            missing.append(p)\n",
    "    if missing:\n",
    "        print(\"Installing missing packages:\", missing)\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + missing\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"Installation finished.\")\n",
    "\n",
    "ensure_packages(REQUIRED)\n",
    "\n",
    "# ---------- Imports (after ensuring packages) ----------\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "\n",
    "# ---------- Paths & timestamps ----------\n",
    "ROOT = os.getcwd()\n",
    "RAW_DIR = os.path.join(ROOT, \"data\", \"raw\")\n",
    "NOTEBOOKS_DIR = os.path.join(ROOT, \"notebooks\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(NOTEBOOKS_DIR, exist_ok=True)\n",
    "\n",
    "TS = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "LOG_PATH = os.path.join(RAW_DIR, f\"ingestion_log_{TS}.txt\")\n",
    "\n",
    "# ---------- Create .env.example if not present ----------\n",
    "ENV_EXAMPLE_PATH = os.path.join(ROOT, \".env.example\")\n",
    "if not os.path.exists(ENV_EXAMPLE_PATH):\n",
    "    with open(ENV_EXAMPLE_PATH, \"w\") as f:\n",
    "        f.write(\"API_KEY=dummy_key_123\\nTICKER=AAPL\\n\")\n",
    "    print(f\"Created .env.example at {ENV_EXAMPLE_PATH} (use this to create your local .env)\")\n",
    "\n",
    "# ---------- Load environment ----------\n",
    "load_dotenv()  # loads .env if present in working directory\n",
    "import os\n",
    "API_KEY = os.getenv(\"API_KEY\")  # Alpha Vantage / other\n",
    "TICKER = os.getenv(\"TICKER\", \"AAPL\")\n",
    "\n",
    "# ---------- Utility: write log ----------\n",
    "def log(msg, to_console=True):\n",
    "    ts = datetime.datetime.now().isoformat()\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    if to_console:\n",
    "        print(line)\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(line + \"\\n\")\n",
    "\n",
    "log(\"Starting ingestion workflow\")\n",
    "\n",
    "# ---------- Validation helpers ----------\n",
    "def validate_api_df(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Validate API DataFrame:\n",
    "    - ensure date column exists and is datetime\n",
    "    - ensure numeric columns (open, high, low, close, volume) exist\n",
    "    - report NA counts and shape\n",
    "    Returns (ok: bool, diagnostics: dict)\n",
    "    \"\"\"\n",
    "    diag = {}\n",
    "    ok = True\n",
    "\n",
    "    # Normalize column names to lower for checking\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    diag['columns'] = list(df.columns)\n",
    "    log(f\"API DataFrame columns: {diag['columns']}\")\n",
    "\n",
    "    # find date column\n",
    "    date_col = None\n",
    "    for candidate in [\"timestamp\", \"date\", \"datetime\", \"time\", \"trade_time\"]:\n",
    "        if candidate in cols_lower:\n",
    "            date_col = cols_lower[candidate]\n",
    "            break\n",
    "    if date_col is None:\n",
    "        # maybe index is datetime; try to reset_index\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            df = df.reset_index()\n",
    "            if isinstance(df.columns[0], str):\n",
    "                date_col = df.columns[0]\n",
    "    if date_col is None:\n",
    "        log(\"Validation error: no date/timestamp column found\")\n",
    "        ok = False\n",
    "        diag['date_error'] = \"no date column\"\n",
    "    else:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        if df[date_col].isna().any():\n",
    "            log(\"Warning: some date parsing failed (NA present)\")\n",
    "        diag['date_col'] = date_col\n",
    "\n",
    "    # required numeric columns (common names)\n",
    "    required = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    found_required = []\n",
    "    for req in required:\n",
    "        # search ignoring case in columns\n",
    "        matches = [c for c in df.columns if c.lower() == req]\n",
    "        if matches:\n",
    "            found_required.append(matches[0])\n",
    "        else:\n",
    "            # also accept variations like 'adj close'\n",
    "            if req == \"close\":\n",
    "                alt = [c for c in df.columns if \"close\" in c.lower()]\n",
    "                if alt:\n",
    "                    found_required.append(alt[0])\n",
    "                    continue\n",
    "            diag[f\"missing_{req}\"] = True\n",
    "            ok = ok and False\n",
    "\n",
    "    diag['found_required'] = found_required\n",
    "    diag['shape'] = df.shape\n",
    "    diag['na_counts'] = df.isna().sum().to_dict()\n",
    "\n",
    "    return ok, diag, df\n",
    "\n",
    "def validate_scrape_df(df: pd.DataFrame, required_cols=None):\n",
    "    diag = {}\n",
    "    ok = True\n",
    "    diag['columns'] = list(df.columns)\n",
    "    if required_cols:\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            diag['missing'] = missing\n",
    "            ok = False\n",
    "    diag['na_counts'] = df.isna().sum().to_dict()\n",
    "    diag['shape'] = df.shape\n",
    "    return ok, diag\n",
    "\n",
    "# ---------- API Pull ----------\n",
    "def fetch_via_alphavantage(ticker, apikey, outputsize=\"compact\"):\n",
    "    \"\"\"\n",
    "    Attempt to fetch daily adjusted time series as CSV from Alpha Vantage.\n",
    "    Returns DataFrame on success, or raises.\n",
    "    \"\"\"\n",
    "    base = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": ticker,\n",
    "        \"outputsize\": outputsize,\n",
    "        \"datatype\": \"csv\",\n",
    "        \"apikey\": apikey\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"stage04-ingest-script/1.0 (python requests)\"}\n",
    "    r = requests.get(base, params=params, headers=headers, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"AlphaVantage request failed: {r.status_code}\")\n",
    "    # the CSV returned has columns like timestamp, open, high, low, close, adjusted close, volume, etc\n",
    "    df = pd.read_csv(io.StringIO(r.text))\n",
    "    return df\n",
    "\n",
    "def fetch_via_yfinance(ticker, period=\"2y\", interval=\"1d\"):\n",
    "    \"\"\"\n",
    "    Fetch history via yfinance\n",
    "    \"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    hist = t.history(period=period, interval=interval, auto_adjust=False)\n",
    "    if hist.empty:\n",
    "        raise RuntimeError(\"yfinance returned empty DataFrame\")\n",
    "    df = hist.reset_index()  # Date -> column\n",
    "    # rename Date->timestamp for compatibility\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df.rename(columns={\"Date\": \"timestamp\"})\n",
    "    return df\n",
    "\n",
    "api_df = None\n",
    "api_source = None\n",
    "api_ok = False\n",
    "api_diag = {}\n",
    "\n",
    "if API_KEY:\n",
    "    log(\"API_KEY found in .env; attempting Alpha Vantage\")\n",
    "    try:\n",
    "        api_df = fetch_via_alphavantage(TICKER, API_KEY)\n",
    "        api_source = \"alphavantage\"\n",
    "        ok, diag, api_df = validate_api_df(api_df)\n",
    "        api_ok = ok\n",
    "        api_diag = diag\n",
    "        if ok:\n",
    "            log(\"Alpha Vantage fetch & validation OK\")\n",
    "        else:\n",
    "            log(\"Alpha Vantage fetch completed but validation failed; details in log\")\n",
    "    except Exception as e:\n",
    "        log(f\"Alpha Vantage fetch failed: {e}. Will fallback to yfinance.\")\n",
    "        API_KEY = None  # force fallback\n",
    "\n",
    "if api_df is None:\n",
    "    log(\"Using yfinance fallback for API pull\")\n",
    "    try:\n",
    "        api_df = fetch_via_yfinance(TICKER)\n",
    "        api_source = \"yfinance\"\n",
    "        ok, diag, api_df = validate_api_df(api_df)\n",
    "        api_ok = ok\n",
    "        api_diag = diag\n",
    "        if ok:\n",
    "            log(\"yfinance fetch & validation OK\")\n",
    "        else:\n",
    "            log(\"yfinance fetch completed but validation flagged issues\")\n",
    "    except Exception as e:\n",
    "        log(f\"yfinance fetch failed: {e}\")\n",
    "        api_df = None\n",
    "\n",
    "# Save API CSV if we have it\n",
    "if api_df is not None:\n",
    "    api_filename = f\"api_{api_source}_{TICKER}_{TS}.csv\"\n",
    "    api_path = os.path.join(RAW_DIR, api_filename)\n",
    "    # ensure date column normalized to 'timestamp' if possible\n",
    "    cols_lower = {c.lower(): c for c in api_df.columns}\n",
    "    if \"timestamp\" in cols_lower:\n",
    "        # ok\n",
    "        pass\n",
    "    else:\n",
    "        # find a date-like column and rename to timestamp\n",
    "        for candidate in [\"date\", \"datetime\"]:\n",
    "            if candidate in cols_lower:\n",
    "                api_df = api_df.rename(columns={cols_lower[candidate]: \"timestamp\"})\n",
    "                break\n",
    "    # coerce numeric columns to floats where possible\n",
    "    for c in api_df.columns:\n",
    "        if c.lower() not in (\"timestamp\", \"date\", \"datetime\", \"symbol\"):\n",
    "            try:\n",
    "                api_df[c] = pd.to_numeric(api_df[c], errors=\"ignore\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    api_df.to_csv(api_path, index=False)\n",
    "    log(f\"Saved API CSV to {api_path}\")\n",
    "    # write diagnostics\n",
    "    with open(os.path.join(RAW_DIR, f\"api_diag_{TS}.txt\"), \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"Source: \" + (api_source or \"none\") + \"\\n\")\n",
    "        fh.write(\"Ticker: \" + TICKER + \"\\n\")\n",
    "        fh.write(\"Validation diag:\\n\")\n",
    "        fh.write(textwrap.indent(str(api_diag), \"  \"))\n",
    "else:\n",
    "    log(\"No API data available; skipping API CSV save\")\n",
    "\n",
    "# ---------- Scrape small table (Wikipedia S&P 500 constituents) ----------\n",
    "SCRAPE_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "scrape_df = None\n",
    "scrape_diag = {}\n",
    "\n",
    "try:\n",
    "    headers = {\"User-Agent\": \"stage04-ingest-script/1.0 (python requests)\"}\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    # Best guess: look for table with id 'constituents' (this matches current WP page)\n",
    "    table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
    "    if table is None:\n",
    "        # Fallback: first wikitable sortable\n",
    "        table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "    if table is None:\n",
    "        raise RuntimeError(\"Couldn't find a suitable table on the page\")\n",
    "\n",
    "    # parse headers\n",
    "    headers_row = table.find(\"tr\")\n",
    "    headers_cols = [th.get_text(strip=True) for th in headers_row.find_all(\"th\")]\n",
    "    # parse data rows\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cols = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "        if len(cols) == 0:\n",
    "            continue\n",
    "        # pad if needed\n",
    "        if len(cols) < len(headers_cols):\n",
    "            cols += [\"\"] * (len(headers_cols) - len(cols))\n",
    "        rows.append(cols[:len(headers_cols)])\n",
    "    scrape_df = pd.DataFrame(rows, columns=headers_cols)\n",
    "\n",
    "    # quick cleaning: common column rename (Symbol -> Symbol)\n",
    "    # Validate that we have a symbol/ticker column\n",
    "    possible_symbol_cols = [c for c in scrape_df.columns if c.lower() in (\"symbol\", \"ticker\", \"ticker symbol\")]\n",
    "    if not possible_symbol_cols:\n",
    "        # try to guess first column as symbol\n",
    "        scrape_df = scrape_df.rename(columns={scrape_df.columns[0]: \"Symbol\"})\n",
    "    # Validate required columns\n",
    "    required = [\"Symbol\", \"Security\"]\n",
    "    ok, diag = validate_scrape_df(scrape_df, required_cols=required)\n",
    "    scrape_diag = diag\n",
    "    if ok:\n",
    "        log(\"Scrape validation OK\")\n",
    "    else:\n",
    "        log(f\"Scrape validation issues: {diag.get('missing', [])}\")\n",
    "\n",
    "    # Save scrape CSV\n",
    "    scrape_filename = f\"scrape_wikipedia_sp500_constituents_{TS}.csv\"\n",
    "    scrape_path = os.path.join(RAW_DIR, scrape_filename)\n",
    "    scrape_df.to_csv(scrape_path, index=False)\n",
    "    log(f\"Saved scraped CSV to {scrape_path}\")\n",
    "\n",
    "    # Save scrape diagnostics\n",
    "    with open(os.path.join(RAW_DIR, f\"scrape_diag_{TS}.txt\"), \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"Source URL: \" + SCRAPE_URL + \"\\n\")\n",
    "        fh.write(\"Validation diag:\\n\")\n",
    "        fh.write(textwrap.indent(str(scrape_diag), \"  \"))\n",
    "except Exception as e:\n",
    "    log(f\"Scraping failed: {e}\")\n",
    "\n",
    "# ---------- Documentation & notebook note ----------\n",
    "doc_path = os.path.join(NOTEBOOKS_DIR, \"stage04_ingestion_notes.md\")\n",
    "doc_text = f\"\"\"\\\n",
    "# Stage 04 â€” Data Acquisition & Ingestion (auto-generated notes)\n",
    "\n",
    "**Timestamp:** {datetime.datetime.now().isoformat()}\n",
    "\n",
    "## API ingestion\n",
    "- Preferred source: Alpha Vantage (CSV) using `TIME_SERIES_DAILY_ADJUSTED` endpoint.\n",
    "- Fallback: `yfinance` Python package.\n",
    "- Config / env:\n",
    "  - .env (local, not committed): API_KEY (Alpha Vantage key), TICKER (e.g., AAPL)\n",
    "  - .env.example is present in repo root.\n",
    "- Example Alpha Vantage params used:\n",
    "  - function=TIME_SERIES_DAILY_ADJUSTED, symbol={TICKER}, outputsize=compact, datatype=csv\n",
    "- Validations performed:\n",
    "  - check presence of a date/timestamp column and convert to datetime\n",
    "  - check for numeric columns: open, high, low, close, volume (or nearest variants)\n",
    "  - report NA counts and DataFrame shape\n",
    "- Output file: data/raw/api_{{source}}_{TICKER}_{TS}.csv\n",
    "\n",
    "## Scraping ingestion\n",
    "- Source URL: {SCRAPE_URL}\n",
    "- Table: S&P 500 constituents (table id='constituents' or first wikitable sortable)\n",
    "- Validation:\n",
    "  - required columns: Symbol, Security\n",
    "  - check NA counts and shape\n",
    "- Output file: data/raw/scrape_wikipedia_sp500_constituents_{TS}.csv\n",
    "\n",
    "## Assumptions & Risks\n",
    "- Alpha Vantage has rate limits (5 calls/minute for free tier). Use API key responsibly.\n",
    "- Wikipedia's HTML structure can change; the scraper uses common selectors and falls back to generic table selection.\n",
    "- Do NOT commit `.env` with real API keys. Use `.env.example` in your repo.\n",
    "- Data freshness depends on API / source availability.\n",
    "\n",
    "## Files created by this run\n",
    "- API CSV, scrape CSV in `data/raw/`\n",
    "- diagnostics text files in `data/raw/`\n",
    "- ingestion log: {os.path.join(RAW_DIR, os.path.basename(LOG_PATH))}\n",
    "\"\"\"\n",
    "\n",
    "with open(doc_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "    fh.write(doc_text)\n",
    "log(f\"Wrote ingestion documentation to {doc_path}\")\n",
    "\n",
    "# ---------- Final summary printed ----------\n",
    "log(\"Ingestion workflow completed\")\n",
    "summary_lines = [\n",
    "    \"Summary of outputs:\",\n",
    "]\n",
    "# list created files in raw dir for this timestamp\n",
    "for fn in os.listdir(RAW_DIR):\n",
    "    if TS in fn:\n",
    "        summary_lines.append(f\" - {os.path.join(RAW_DIR, fn)}\")\n",
    "for line in summary_lines:\n",
    "    log(line)\n",
    "\n",
    "# end of script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e91d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
