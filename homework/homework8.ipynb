{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9dfdbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDA] Using dataset: C:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\processed\\amazon_bestsellers_2025_cleaned.csv\n",
      "\n",
      "✅ EDA complete.\n",
      "   Report: c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\notebooks\\eda_report.md\n",
      "   Figures: c:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\notebooks\\figures\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Stage 08 — Exploratory Data Analysis (EDA)\n",
    "- Loads processed dataset (prefers Stage 06 cleaned file; else first CSV in processed dir)\n",
    "- Profiles numeric & categorical columns\n",
    "- Plots ≥3 distributions and ≥2 relationships\n",
    "- Optional correlation heatmap\n",
    "- Notes skew/outliers/seasonality/structure\n",
    "- Writes Markdown report + saves figures\n",
    "\n",
    "Outputs:\n",
    "  notebooks/eda_report.md\n",
    "  notebooks/figures/*.png\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import textwrap\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths & loading helpers\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# Prefer env from earlier stages; fallback to your Windows path you shared\n",
    "DATA_DIR_PROCESSED = os.getenv(\n",
    "    \"DATA_DIR_PROCESSED\",\n",
    "    r\"C:\\Users\\sarda\\Desktop\\bootcamp_darshit_sarda\\homework\\data\\processed\",\n",
    ")\n",
    "REPO_NOTEBOOKS = os.path.join(os.getcwd(), \"notebooks\")\n",
    "FIG_DIR = os.path.join(REPO_NOTEBOOKS, \"figures\")\n",
    "os.makedirs(REPO_NOTEBOOKS, exist_ok=True)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Preferred file from Stage 06 (cleaned dataset), else first CSV\n",
    "PREFERRED = os.path.join(DATA_DIR_PROCESSED, \"amazon_bestsellers_2025_cleaned.csv\")\n",
    "if os.path.exists(PREFERRED):\n",
    "    DATA_PATH = PREFERRED\n",
    "else:\n",
    "    csvs = sorted(glob.glob(os.path.join(DATA_DIR_PROCESSED, \"*.csv\")))\n",
    "    if not csvs:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No CSVs found in processed dir.\\nChecked: {DATA_DIR_PROCESSED}\\n\"\n",
    "            \"Make sure Stage 05/06 produced a CSV in data/processed.\"\n",
    "        )\n",
    "    DATA_PATH = csvs[0]\n",
    "\n",
    "print(f\"[EDA] Using dataset: {DATA_PATH}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Try to parse date-like columns by name\n",
    "DATE_CANDIDATES = [\"date\", \"Date\", \"timestamp\", \"Timestamp\", \"datetime\", \"Datetime\"]\n",
    "for col in df.columns:\n",
    "    if any(key in str(col).lower() for key in [\"date\", \"time\"]):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            try:\n",
    "                parsed = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
    "                # Consider it a date column if at least 80% parsed\n",
    "                if parsed.notna().mean() > 0.8:\n",
    "                    df[col] = parsed\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# Identify a primary date column if present\n",
    "date_col = None\n",
    "for c in df.columns:\n",
    "    if np.issubdtype(df[c].dtype, np.datetime64):\n",
    "        date_col = c\n",
    "        break\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Profile numeric/categorical\n",
    "# -----------------------------\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number, \"datetime64[ns]\"]).columns.tolist()\n",
    "\n",
    "# Capture df.info() into a string\n",
    "buf = io.StringIO()\n",
    "df.info(buf=buf)\n",
    "info_text = buf.getvalue()\n",
    "\n",
    "describe_num = df[numeric_cols].describe().T if numeric_cols else pd.DataFrame()\n",
    "describe_all = df.describe(include=\"all\").T\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Plots — distributions\n",
    "# -----------------------------\n",
    "def safe_filename(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in (\"-\", \"_\") else \"_\" for ch in name)\n",
    "\n",
    "# Pick up to 3 numeric columns for hist & box\n",
    "dist_cols = numeric_cols[:3] if len(numeric_cols) >= 3 else numeric_cols\n",
    "\n",
    "saved_figs = []\n",
    "\n",
    "# Histograms (≥3 if possible)\n",
    "if dist_cols:\n",
    "    for col in dist_cols:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        data = df[col].dropna()\n",
    "        plt.hist(data, bins=30)\n",
    "        plt.title(f\"Distribution — {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        out = os.path.join(FIG_DIR, f\"hist_{safe_filename(col)}.png\")\n",
    "        plt.savefig(out, dpi=150)\n",
    "        plt.close()\n",
    "        saved_figs.append(out)\n",
    "\n",
    "# Boxplots for same cols\n",
    "if dist_cols:\n",
    "    for col in dist_cols:\n",
    "        plt.figure(figsize=(6, 2.8))\n",
    "        data = df[col].dropna()\n",
    "        plt.boxplot(data, vert=False)\n",
    "        plt.title(f\"Boxplot — {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.tight_layout()\n",
    "        out = os.path.join(FIG_DIR, f\"box_{safe_filename(col)}.png\")\n",
    "        plt.savefig(out, dpi=150)\n",
    "        plt.close()\n",
    "        saved_figs.append(out)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Bivariate visuals (≥2)\n",
    "# -----------------------------\n",
    "# Scatter: pick two numeric columns if available\n",
    "scatter_path = None\n",
    "if len(numeric_cols) >= 2:\n",
    "    xcol, ycol = numeric_cols[0], numeric_cols[1]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df[xcol], df[ycol], alpha=0.7)\n",
    "    plt.title(f\"Scatter — {xcol} vs {ycol}\")\n",
    "    plt.xlabel(xcol)\n",
    "    plt.ylabel(ycol)\n",
    "    plt.tight_layout()\n",
    "    scatter_path = os.path.join(FIG_DIR, f\"scatter_{safe_filename(xcol)}_vs_{safe_filename(ycol)}.png\")\n",
    "    plt.savefig(scatter_path, dpi=150)\n",
    "    plt.close()\n",
    "    saved_figs.append(scatter_path)\n",
    "\n",
    "# Line/time series: if date_col exists + at least one numeric col\n",
    "ts_path = None\n",
    "if date_col and len(numeric_cols) >= 1:\n",
    "    y_ts = numeric_cols[0]\n",
    "    # Sort by date for clean plotting\n",
    "    df_ts = df[[date_col, y_ts]].dropna().sort_values(by=date_col)\n",
    "    if not df_ts.empty:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(df_ts[date_col], df_ts[y_ts])\n",
    "        plt.title(f\"Time Series — {y_ts} over {date_col}\")\n",
    "        plt.xlabel(date_col)\n",
    "        plt.ylabel(y_ts)\n",
    "        plt.xticks(rotation=20)\n",
    "        plt.tight_layout()\n",
    "        ts_path = os.path.join(FIG_DIR, f\"timeseries_{safe_filename(y_ts)}.png\")\n",
    "        plt.savefig(ts_path, dpi=150)\n",
    "        plt.close()\n",
    "        saved_figs.append(ts_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Correlation heatmap (optional)\n",
    "# -----------------------------\n",
    "heatmap_path = None\n",
    "if len(numeric_cols) >= 2:\n",
    "    corr = df[numeric_cols].corr()\n",
    "    plt.figure(figsize=(0.9 * len(numeric_cols) + 3, 0.9 * len(numeric_cols) + 3))\n",
    "    im = plt.imshow(corr, interpolation=\"nearest\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(numeric_cols)), numeric_cols)\n",
    "    plt.tight_layout()\n",
    "    heatmap_path = os.path.join(FIG_DIR, \"correlation_heatmap.png\")\n",
    "    plt.savefig(heatmap_path, dpi=150)\n",
    "    plt.close()\n",
    "    saved_figs.append(heatmap_path)\n",
    "else:\n",
    "    corr = pd.DataFrame()\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Automated insights (“so what?”)\n",
    "# -----------------------------\n",
    "insights = []\n",
    "\n",
    "# 6.1 Most skewed numeric features\n",
    "if numeric_cols:\n",
    "    skews = df[numeric_cols].skew(numeric_only=True).abs().sort_values(ascending=False)\n",
    "    top_skew = skews.head(3)\n",
    "    if not top_skew.empty:\n",
    "        skew_lines = [f\"- **{idx}** | |skew| = {val:.2f}\" for idx, val in top_skew.items()]\n",
    "        insights.append(\"**Skewness:**\\n\" + \"\\n\".join(skew_lines))\n",
    "\n",
    "# 6.2 Highest correlation pairs\n",
    "if not corr.empty and corr.shape[0] >= 2:\n",
    "    corr_upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    corr_pairs = corr_upper.unstack().dropna().abs().sort_values(ascending=False)\n",
    "    top_corrs = corr_pairs.head(3)\n",
    "    if not top_corrs.empty:\n",
    "        lines = []\n",
    "        for (a, b), v in top_corrs.items():\n",
    "            lines.append(f\"- **{a} ~ {b}**: |r| = {v:.2f}\")\n",
    "        insights.append(\"**Strongest correlations (abs):**\\n\" + \"\\n\".join(lines))\n",
    "\n",
    "# 6.3 Missingness\n",
    "if missing_counts.sum() > 0:\n",
    "    miss_top = missing_counts[missing_counts > 0].head(5)\n",
    "    lines = [f\"- **{idx}**: {val} missing\" for idx, val in miss_top.items()]\n",
    "    insights.append(\"**Missing values (top):**\\n\" + \"\\n\".join(lines))\n",
    "\n",
    "# 6.4 Time series note\n",
    "if date_col:\n",
    "    insights.append(f\"**Temporal structure:** Found a date column (**{date_col}**). Consider seasonality/trends.\")\n",
    "\n",
    "# Fallback insight if nothing above\n",
    "if not insights:\n",
    "    insights.append(\"**Data appears relatively clean with limited skew/missingness.**\")\n",
    "\n",
    "# Top 3 insights selection\n",
    "top3 = insights[:3]\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Implications for next step\n",
    "# -----------------------------\n",
    "implications = [\n",
    "    \"- Apply transformations to high-skew features (e.g., log/Box-Cox) before modeling.\",\n",
    "    \"- Address multicollinearity (remove or combine highly correlated features) to stabilize models.\",\n",
    "    \"- Impute or drop columns with substantial missingness; document rationale.\",\n",
    "]\n",
    "if date_col:\n",
    "    implications.append(\"- Engineer calendar/time features (month, quarter, day-of-week, lag/rolling stats).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Assemble Markdown report\n",
    "# -----------------------------\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "report_path = os.path.join(REPO_NOTEBOOKS, \"eda_report.md\")\n",
    "\n",
    "def df_to_markdown_table(d: pd.DataFrame, max_rows=30, max_cols=12) -> str:\n",
    "    if d is None or d.empty:\n",
    "        return \"_(empty)_\"\n",
    "    d2 = d.copy()\n",
    "    if d2.shape[0] > max_rows:\n",
    "        d2 = d2.head(max_rows)\n",
    "    if d2.shape[1] > max_cols:\n",
    "        d2 = d2.iloc[:, :max_cols]\n",
    "    return d2.to_markdown()\n",
    "\n",
    "md = []\n",
    "\n",
    "md.append(f\"# Exploratory Data Analysis (Stage 08)\\n\")\n",
    "md.append(f\"_Generated: {ts}_\\n\")\n",
    "md.append(f\"**Dataset:** `{DATA_PATH}`  \\n\")\n",
    "md.append(f\"**Rows × Cols:** {df.shape[0]} × {df.shape[1]}\\n\")\n",
    "\n",
    "md.append(\"## 1) Statistical Summaries\")\n",
    "md.append(\"### `df.info()`\")\n",
    "md.append(\"```text\\n\" + info_text.strip() + \"\\n```\")\n",
    "\n",
    "md.append(\"### `df.describe()` (numeric)\")\n",
    "md.append(df_to_markdown_table(describe_num))\n",
    "\n",
    "md.append(\"### Missing value counts\")\n",
    "md.append(df_to_markdown_table(missing_counts.to_frame(name=\"missing\")))\n",
    "\n",
    "md.append(\"### Column types\")\n",
    "md.append(f\"- Numeric columns ({len(numeric_cols)}): {', '.join(numeric_cols) if numeric_cols else '—'}\")\n",
    "md.append(f\"- Categorical columns ({len(categorical_cols)}): {', '.join(categorical_cols) if categorical_cols else '—'}\")\n",
    "md.append(f\"- Date column: {date_col if date_col else '—'}\")\n",
    "\n",
    "md.append(\"## 2) Distributions (Histograms & Boxplots)\")\n",
    "for p in saved_figs:\n",
    "    if os.path.basename(p).startswith(\"hist_\") or os.path.basename(p).startswith(\"box_\"):\n",
    "        md.append(f\"![{os.path.basename(p)}](figures/{os.path.basename(p)})\")\n",
    "\n",
    "md.append(\"## 3) Bivariate Visuals\")\n",
    "if scatter_path:\n",
    "    md.append(f\"**Scatter plot:**\\n\\n![{os.path.basename(scatter_path)}](figures/{os.path.basename(scatter_path)})\")\n",
    "else:\n",
    "    md.append(\"_Not enough numeric columns for scatter plot._\")\n",
    "\n",
    "if ts_path:\n",
    "    md.append(f\"**Time series plot:**\\n\\n![{os.path.basename(ts_path)}](figures/{os.path.basename(ts_path)})\")\n",
    "else:\n",
    "    md.append(\"_No date column or suitable numeric series for time plot._\")\n",
    "\n",
    "if heatmap_path:\n",
    "    md.append(\"## 4) Correlation Heatmap (Optional)\")\n",
    "    md.append(f\"![{os.path.basename(heatmap_path)}](figures/{os.path.basename(heatmap_path)})\")\n",
    "\n",
    "md.append(\"## 5) Findings: Skew, Outliers, Seasonality, Structure\")\n",
    "md.extend([f\"- {line}\" for line in insights])\n",
    "\n",
    "md.append(\"## 6) Implications for Next Step\")\n",
    "md.extend([f\"- {line}\" for line in implications])\n",
    "\n",
    "md.append(\"## Top 3 Insights\")\n",
    "for i, line in enumerate(top3, 1):\n",
    "    md.append(f\"{i}. {line}\")\n",
    "\n",
    "md.append(\"## Assumptions & Risks\")\n",
    "md.append(\n",
    "    textwrap.dedent(\n",
    "        \"\"\"\n",
    "        - Missingness is assumed to be at random; if not, imputation may bias results.\n",
    "        - High correlations can inflate variance of coefficients; monitor VIF or use regularization.\n",
    "        - If outliers are business-meaningful events, trimming/winsorizing could harm signal.\n",
    "        - Correlation ≠ causation; domain validation is required before feature removal.\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(md))\n",
    "\n",
    "print(f\"\\n✅ EDA complete.\")\n",
    "print(f\"   Report: {report_path}\")\n",
    "print(f\"   Figures: {FIG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf426f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf59e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
